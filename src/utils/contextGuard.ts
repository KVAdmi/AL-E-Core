/**
 * Context Window Guard - Previene OpenAI 400 por exceso de tokens
 * PRIORIDAD: identidad > historial > chunks
 * 
 * DISEÃ‘O DETERMINISTA:
 * - No usa estimaciÃ³n de tokens (poco confiable)
 * - Usa lÃ­mites por longitud de caracteres
 * - LÃ­mites configurables por env
 */

import { AssistantMessage } from '../ai/IAssistantProvider';

// ðŸ”’ LÃMITES CONFIGURABLES (deterministas, no estimados)
const OPENAI_CONTEXT_LIMIT = parseInt(process.env.OPENAI_CONTEXT_LIMIT || '16000', 10);
const OPENAI_MAX_OUTPUT_TOKENS = parseInt(process.env.OPENAI_MAX_OUTPUT_TOKENS || '500', 10);

// LÃ­mites por longitud de caracteres (determinista)
const MAX_MESSAGES_HISTORY = 12; // MÃ¡ximo 12 mensajes histÃ³ricos
const MAX_CHARS_PER_MESSAGE = 2000; // MÃ¡ximo 2000 chars por mensaje
const MAX_CHUNKS = 3; // MÃ¡ximo 3 chunks
const MAX_CHARS_PER_CHUNK = 2000; // MÃ¡ximo 2000 chars por chunk

interface ContextGuardResult {
  messages: AssistantMessage[];
  systemPrompt: string;
  chunks: string;
  identityPreserved: boolean;
  wasTruncated: boolean;
}

export function guardContextWindow(
  messages: AssistantMessage[],
  systemPrompt: string,
  chunks: string = '',
  identityInjected: boolean = false
): ContextGuardResult {
  
  console.log(`[CONTEXT GUARD] Aplicando recorte determinista (max ${MAX_MESSAGES_HISTORY} msgs, ${MAX_CHUNKS} chunks)`);
  
  let truncated = false;
  
  // 1ï¸âƒ£ SYSTEM PROMPT / IDENTIDAD (NUNCA TRUNCAR)
  const finalSystemPrompt = systemPrompt;
  console.log(`[CONTEXT GUARD] System prompt preservado (${systemPrompt.length} chars, identity=${identityInjected})`);
  
  // 2ï¸âƒ£ CHUNKS (TRUNCAR PRIMERO - PRIORIDAD BAJA)
  let finalChunks = '';
  if (chunks && chunks.length > 0) {
    // Truncar por caracteres (no tokens)
    const maxChunkLength = MAX_CHUNKS * MAX_CHARS_PER_CHUNK;
    
    if (chunks.length <= maxChunkLength) {
      finalChunks = chunks;
    } else {
      finalChunks = chunks.substring(0, maxChunkLength) + '... [truncado]';
      truncated = true;
      console.log(`[CONTEXT GUARD] Chunks truncados: ${chunks.length} â†’ ${maxChunkLength} chars`);
    }
  }
  
  // 3ï¸âƒ£ HISTORIAL (RECORTAR DESDE EL MÃS VIEJO)
  let finalMessages = [...messages];
  
  // LÃ­mite estricto: mÃ¡ximo 12 mensajes
  if (finalMessages.length > MAX_MESSAGES_HISTORY) {
    const removed = finalMessages.length - MAX_MESSAGES_HISTORY;
    finalMessages = finalMessages.slice(-MAX_MESSAGES_HISTORY); // Ãšltimos 12
    truncated = true;
    console.log(`[CONTEXT GUARD] Historial reducido: removidos ${removed} mensajes mÃ¡s antiguos`);
  }
  
  // Truncar cada mensaje por longitud de caracteres
  finalMessages = finalMessages.map((msg, idx) => {
    const msgContent = typeof msg.content === 'string' 
      ? msg.content 
      : JSON.stringify(msg.content);
    
    if (msgContent.length > MAX_CHARS_PER_MESSAGE) {
      truncated = true;
      console.log(`[CONTEXT GUARD] Mensaje ${idx} truncado: ${msgContent.length} â†’ ${MAX_CHARS_PER_MESSAGE} chars`);
      
      return {
        ...msg,
        content: msgContent.substring(0, MAX_CHARS_PER_MESSAGE) + '... [truncado]'
      };
    }
    
    return msg;
  });
  
  console.log(`[CONTEXT GUARD] Resultado: ${finalMessages.length} msgs, ${finalChunks.length} chars de chunks (truncado: ${truncated})`);
  
  return {
    messages: finalMessages,
    systemPrompt: finalSystemPrompt,
    chunks: finalChunks,
    identityPreserved: identityInjected,
    wasTruncated: truncated
  };
}

export function getMaxOutputTokens(): number {
  return OPENAI_MAX_OUTPUT_TOKENS;
}
